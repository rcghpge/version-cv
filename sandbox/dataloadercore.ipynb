{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9a66847-fedf-4007-9f65-13c0680fbfe6",
   "metadata": {},
   "source": [
    "# dataloader core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676aa828-91dd-4af7-91a3-9b696046a8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import sympy\n",
    "import random\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from typing import Optional, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# ---------------------------\n",
    "# Image Data Generator Loader\n",
    "# ---------------------------\n",
    "def create_image_generators(\n",
    "    data_dir: Optional[str] = None,\n",
    "    dataframe: Optional[pd.DataFrame] = None,\n",
    "    x_col: str = \"filename\",\n",
    "    y_col: str = \"class\",\n",
    "    img_size: Tuple[int, int] = (224, 224),\n",
    "    batch_size: int = 32,\n",
    "    val_split: float = 0.2,\n",
    "    augment: bool = False,\n",
    "    class_mode: str = \"categorical\"\n",
    "):\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        validation_split=val_split,\n",
    "        rotation_range=10 if augment else 0,\n",
    "        zoom_range=0.1 if augment else 0,\n",
    "        horizontal_flip=augment,\n",
    "    )\n",
    "\n",
    "    if dataframe is not None:\n",
    "        train_gen = datagen.flow_from_dataframe(\n",
    "            dataframe=dataframe,\n",
    "            directory=data_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_col,\n",
    "            target_size=img_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode=class_mode,\n",
    "            subset=\"training\",\n",
    "            shuffle=True,\n",
    "            seed=42,\n",
    "        )\n",
    "        val_gen = datagen.flow_from_dataframe(\n",
    "            dataframe=dataframe,\n",
    "            directory=data_dir,\n",
    "            x_col=x_col,\n",
    "            y_col=y_col,\n",
    "            target_size=img_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode=class_mode,\n",
    "            subset=\"validation\",\n",
    "            shuffle=False,\n",
    "            seed=42,\n",
    "        )\n",
    "    else:\n",
    "        if data_dir is None:\n",
    "            raise ValueError(\"Either `dataframe` or `data_dir` must be provided.\")\n",
    "\n",
    "        train_gen = datagen.flow_from_directory(\n",
    "            data_dir,\n",
    "            target_size=img_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode=class_mode,\n",
    "            subset=\"training\",\n",
    "            shuffle=True,\n",
    "            seed=42,\n",
    "        )\n",
    "        val_gen = datagen.flow_from_directory(\n",
    "            data_dir,\n",
    "            target_size=img_size,\n",
    "            batch_size=batch_size,\n",
    "            class_mode=class_mode,\n",
    "            subset=\"validation\",\n",
    "            shuffle=False,\n",
    "            seed=42,\n",
    "        )\n",
    "\n",
    "    return train_gen, val_gen\n",
    "\n",
    "# ---------------------------\n",
    "# Parquet Loader\n",
    "# ---------------------------\n",
    "def load_parquet(\n",
    "    parquet_path: str,\n",
    "    target_column: Optional[str] = None,\n",
    "    feature_columns: Optional[list[str]] = None,\n",
    "    test_size: float = 0.2,\n",
    "    val_size: float = 0.1,\n",
    "    random_state: int = 42,\n",
    "    verbose: bool = True\n",
    "):\n",
    "    parquet_path = os.path.expanduser(parquet_path)\n",
    "    if \"*\" in parquet_path or \"?\" in parquet_path or \"[\" in parquet_path:\n",
    "        file_list = glob(parquet_path, recursive=True)\n",
    "        if not file_list:\n",
    "            raise FileNotFoundError(f\"No parquet files matched: {parquet_path}\")\n",
    "        if verbose:\n",
    "            print(f\"Found {len(file_list)} parquet files.\")\n",
    "        df_list = [pd.read_parquet(f) for f in file_list]\n",
    "        df = pd.concat(df_list, ignore_index=True)\n",
    "    else:\n",
    "        df = pd.read_parquet(parquet_path)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Final shape: {df.shape}\")\n",
    "\n",
    "    if not target_column:\n",
    "        return df, None, None, None, None\n",
    "\n",
    "    X = df[feature_columns] if feature_columns else df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(test_size + val_size), random_state=random_state)\n",
    "    rel_val_size = val_size / (test_size + val_size)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=rel_val_size, random_state=random_state)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "    return X_train, X_val, y_train, y_val, X_test\n",
    "\n",
    "# ---------------------------\n",
    "# OCR Block\n",
    "# ---------------------------\n",
    "def ocr_extract_text(image_path: str, vectorizer: Optional[TfidfVectorizer] = None, verbose: bool = True):\n",
    "    img = Image.open(image_path)\n",
    "    text = pytesseract.image_to_string(img)\n",
    "    if verbose:\n",
    "        print(f\"OCR Text Preview:\\n{text[:300]}...\")\n",
    "    if vectorizer:\n",
    "        text_vector = vectorizer.transform([text])\n",
    "        return text, text_vector\n",
    "    else:\n",
    "        return text, None\n",
    "\n",
    "# ---------------------------\n",
    "# InkML Parser and Loader\n",
    "# ---------------------------\n",
    "def parse_inkml_file(file_path, use_time=True, normalize=True):\n",
    "    import lxml.etree as ET\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    label_elem = root.find(\".//{http://www.w3.org/2003/InkML}annotation[@type='normalizedLabel']\")\n",
    "    norm_label = label_elem.text if label_elem is not None else None\n",
    "\n",
    "    split_elem = root.find(\".//{http://www.w3.org/2003/InkML}annotation[@type='splitTagOriginal']\")\n",
    "    split_tag = split_elem.text.lower() if split_elem is not None else \"train\"\n",
    "\n",
    "    traces = []\n",
    "    for trace in root.findall(\".//{http://www.w3.org/2003/InkML}trace\"):\n",
    "        coords_strs = trace.text.strip().split(\",\")\n",
    "        trace_coords = []\n",
    "        for coord_str in coords_strs:\n",
    "            points = coord_str.strip().split(\" \")\n",
    "            points = [p for p in points if p != \"\"]\n",
    "            if use_time and len(points) >= 3:\n",
    "                x, y, t = float(points[0]), float(points[1]), float(points[2])\n",
    "                trace_coords.append([x, y, t])\n",
    "            elif not use_time and len(points) >= 2:\n",
    "                x, y = float(points[0]), float(points[1])\n",
    "                trace_coords.append([x, y])\n",
    "        if trace_coords:\n",
    "            traces.append(np.array(trace_coords))\n",
    "\n",
    "    if normalize and traces:\n",
    "        all_points = np.vstack([t[:, :2] for t in traces if t.shape[0] > 0])\n",
    "        min_vals = np.min(all_points, axis=0)\n",
    "        max_vals = np.max(all_points, axis=0)\n",
    "        for i, t in enumerate(traces):\n",
    "            t[:, :2] = (t[:, :2] - min_vals) / (max_vals - min_vals + 1e-8)\n",
    "            traces[i] = t\n",
    "\n",
    "    return norm_label, traces, split_tag\n",
    "\n",
    "def pad_trace_sequences(trace_list, max_len=300):\n",
    "    flat_coords = []\n",
    "    for trace in trace_list:\n",
    "        flat_trace = np.concatenate(trace, axis=0) if len(trace) > 0 else np.zeros((1, 2))\n",
    "        flat_coords.append(flat_trace)\n",
    "    padded = pad_sequences(flat_coords, maxlen=max_len, dtype=\"float32\", padding=\"post\", truncating=\"post\")\n",
    "    return padded\n",
    "\n",
    "def load_inkml_pipeline(\n",
    "    inkml_dir: str,\n",
    "    tokenizer_name: str = \"gpt2\",\n",
    "    max_length: int = 64,\n",
    "    use_time: bool = True,\n",
    "    normalize: bool = True,\n",
    "    pad_traces: bool = True,\n",
    "    max_trace_len: int = 300,\n",
    "    batch_size: int = 32,\n",
    "    verbose: bool = True\n",
    "):\n",
    "    from collections import defaultdict\n",
    "    inkml_files = glob(os.path.join(inkml_dir, \"*.inkml\"))\n",
    "    splits = defaultdict(list)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    for file_path in inkml_files:\n",
    "        norm_label, traces, split_tag = parse_inkml_file(file_path, use_time=use_time, normalize=normalize)\n",
    "        if norm_label is None or not traces:\n",
    "            continue\n",
    "\n",
    "        tokens = tokenizer(norm_label, truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "        encoded_latex = tokens[\"input_ids\"]\n",
    "\n",
    "        sample = {\n",
    "            \"traces\": [t.tolist() for t in traces],\n",
    "            \"latex_ids\": encoded_latex,\n",
    "            \"normalized_label\": norm_label,\n",
    "            \"file_path\": file_path\n",
    "        }\n",
    "        splits[split_tag].append(sample)\n",
    "\n",
    "    if verbose:\n",
    "        for split_name in splits:\n",
    "            print(f\"{split_name}: {len(splits[split_name])} samples\")\n",
    "\n",
    "    dataset_dict = {split_name: Dataset.from_list(samples) for split_name, samples in splits.items()}\n",
    "    dataset_dict = DatasetDict(dataset_dict)\n",
    "\n",
    "    if pad_traces:\n",
    "        def pad_and_prepare(example):\n",
    "            padded_traces = pad_trace_sequences([example[\"traces\"]], max_len=max_trace_len)[0]\n",
    "            example[\"padded_traces\"] = padded_traces.tolist()\n",
    "            return example\n",
    "\n",
    "        for split_name in dataset_dict:\n",
    "            dataset_dict[split_name] = dataset_dict[split_name].map(pad_and_prepare)\n",
    "\n",
    "    tf_datasets = {}\n",
    "    for split_name in dataset_dict:\n",
    "        tf_dataset = dataset_dict[split_name].to_tf_dataset(\n",
    "            columns=[\"padded_traces\", \"latex_ids\"],\n",
    "            shuffle=(split_name == \"train\"),\n",
    "            batch_size=batch_size,\n",
    "        )\n",
    "        tf_datasets[split_name] = tf_dataset\n",
    "\n",
    "    return dataset_dict, tf_datasets\n",
    "\n",
    "# ---------------------------\n",
    "# Hugging Face Dataset (Image + LaTeX)\n",
    "# ---------------------------\n",
    "def load_datasets_pipeline(\n",
    "    dataset_name: str = \"deepcopy/MathWriting-Human\",\n",
    "    tokenizer_name: str = \"gpt2\",\n",
    "    image_size: tuple = (224, 224),\n",
    "    batch_size: int = 32,\n",
    "    shuffle: bool = True,\n",
    "    num_proc: int = 12,\n",
    "    max_length: int = 64,\n",
    "    verbose: bool = True\n",
    "):\n",
    "    ds = load_dataset(dataset_name)\n",
    "    if verbose:\n",
    "        print(ds)\n",
    "\n",
    "    def resize_image(example, size=image_size):\n",
    "        example[\"image\"] = example[\"image\"].resize(size)\n",
    "        return example\n",
    "\n",
    "    ds[\"train\"] = ds[\"train\"].map(resize_image, num_proc=num_proc)\n",
    "    ds[\"val\"] = ds[\"val\"].map(resize_image, num_proc=num_proc)\n",
    "    ds[\"test\"] = ds[\"test\"].map(resize_image, num_proc=num_proc)\n",
    "\n",
    "    latex_pool = ds[\"train\"][\"latex\"]\n",
    "\n",
    "    def add_binary_label(example, latex_list):\n",
    "        if random.random() > 0.5:\n",
    "            example[\"label\"] = 1\n",
    "            example[\"latex_used\"] = example[\"latex\"]\n",
    "        else:\n",
    "            wrong_latex = random.choice(latex_list)\n",
    "            while wrong_latex == example[\"latex\"]:\n",
    "                wrong_latex = random.choice(latex_list)\n",
    "            example[\"label\"] = 0\n",
    "            example[\"latex_used\"] = wrong_latex\n",
    "        return example\n",
    "\n",
    "    ds[\"train\"] = ds[\"train\"].map(lambda x: add_binary_label(x, latex_pool), num_proc=num_proc)\n",
    "    ds[\"val\"] = ds[\"val\"].map(lambda x: add_binary_label(x, latex_pool), num_proc=num_proc)\n",
    "    ds[\"test\"] = ds[\"test\"].map(lambda x: add_binary_label(x, latex_pool), num_proc=num_proc)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    def tokenize_latex(example):\n",
    "        tokens = tokenizer(example[\"latex_used\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "        example[\"latex_ids\"] = tokens[\"input_ids\"]\n",
    "        return example\n",
    "\n",
    "    ds[\"train\"] = ds[\"train\"].map(tokenize_latex, num_proc=num_proc)\n",
    "    ds[\"val\"] = ds[\"val\"].map(tokenize_latex, num_proc=num_proc)\n",
    "    ds[\"test\"] = ds[\"test\"].map(tokenize_latex, num_proc=num_proc)\n",
    "\n",
    "    tf_train = ds[\"train\"].to_tf_dataset(columns=[\"image\", \"latex_ids\"], label_cols=[\"label\"], shuffle=shuffle, batch_size=batch_size)\n",
    "    tf_val = ds[\"val\"].to_tf_dataset(columns=[\"image\", \"latex_ids\"], label_cols=[\"label\"], shuffle=False, batch_size=batch_size)\n",
    "    tf_test = ds[\"test\"].to_tf_dataset(columns=[\"image\", \"latex_ids\"], label_cols=[\"label\"], shuffle=False, batch_size=batch_size)\n",
    "\n",
    "    return tf_train, tf_val, tf_test\n",
    "\n",
    "# ---------------------------\n",
    "# Text Pipeline (Vectorizer or Tokenized)\n",
    "# ---------------------------\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"SpaCy model not found. Please run: python -m spacy download en_core_web_sm\")\n",
    "    nlp = None\n",
    "\n",
    "def load_text_pipeline(\n",
    "    base_path: str,\n",
    "    vectorizer: TfidfVectorizer = None,\n",
    "    use_sympy: bool = False,\n",
    "    max_features: int = 10000,\n",
    "    stop_words: str = \"english\",\n",
    "    verbose: bool = True\n",
    "):\n",
    "    paths = glob.glob(os.path.join(base_path, \"*.txt\"))\n",
    "    docs, labels = [], []\n",
    "\n",
    "    for path in paths:\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            docs.append(f.read())\n",
    "        label = os.path.basename(path).split(\"__\")[0]\n",
    "        labels.append(label)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Loaded {len(docs)} docs from {base_path}\")\n",
    "\n",
    "    processed_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        if nlp:\n",
    "            spacy_doc = nlp(doc)\n",
    "            tokens = [token.text for token in spacy_doc if not token.is_space]\n",
    "            if stop_words == \"english\":\n",
    "                tokens = [t for t in tokens if not t.lower() in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "        else:\n",
    "            tokens = doc.split()\n",
    "\n",
    "        if use_sympy:\n",
    "            try:\n",
    "                expr = sympy.sympify(doc)\n",
    "                sym_tokens = [str(s) for s in expr.free_symbols]\n",
    "                tokens.extend(sym_tokens)\n",
    "            except sympy.SympifyError:\n",
    "                pass\n",
    "\n",
    "        processed_docs.append(\" \".join(tokens))\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(labels)\n",
    "\n",
    "    if vectorizer is None:\n",
    "        X = [doc.split() for doc in processed_docs]\n",
    "        vec = None\n",
    "        if verbose:\n",
    "            print(\"Tokenized using SpaCy/SymPy (no vectorizer). Classes:\", list(le.classes_))\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(max_features=max_features, stop_words=None)\n",
    "        vectorizer.fit(processed_docs)\n",
    "        X = vectorizer.transform(processed_docs)\n",
    "        vec = vectorizer\n",
    "        if verbose:\n",
    "            print(\"Vectorized. X shape:\", X.shape, \"Classes:\", list(le.classes_))\n",
    "\n",
    "    return X, y, le, vec\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
