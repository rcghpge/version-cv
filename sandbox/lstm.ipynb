{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae90df0a-aaa3-4e1d-bef2-6af6e4d9b3b7",
   "metadata": {},
   "source": [
    "# lstm core from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801332c5-bb83-4312-a21c-056c1f66d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.src import activations\n",
    "from keras.src import backend\n",
    "from keras.src import constraints\n",
    "from keras.src import initializers\n",
    "from keras.src import ops\n",
    "from keras.src import regularizers\n",
    "from keras.src import tree\n",
    "from keras.src.api_export import keras_export\n",
    "from keras.src.layers.input_spec import InputSpec\n",
    "from keras.src.layers.layer import Layer\n",
    "from keras.src.layers.rnn.dropout_rnn_cell import DropoutRNNCell\n",
    "from keras.src.layers.rnn.rnn import RNN\n",
    "\n",
    "\n",
    "@keras_export(\"keras.layers.LSTMCell\")\n",
    "class LSTMCell(Layer, DropoutRNNCell):\n",
    "    \"\"\"Cell class for the LSTM layer.\n",
    "\n",
    "    This class processes one step within the whole time sequence input, whereas\n",
    "    `keras.layer.LSTM` processes the whole sequence.\n",
    "\n",
    "    Args:\n",
    "        units: Positive integer, dimensionality of the output space.\n",
    "        activation: Activation function to use. Default: hyperbolic tangent\n",
    "            (`tanh`). If you pass None, no activation is applied\n",
    "            (ie. \"linear\" activation: `a(x) = x`).\n",
    "        recurrent_activation: Activation function to use for the recurrent step.\n",
    "            Default: sigmoid (`sigmoid`). If you pass `None`, no activation is\n",
    "            applied (ie. \"linear\" activation: `a(x) = x`).\n",
    "        use_bias: Boolean, (default `True`), whether the layer\n",
    "            should use a bias vector.\n",
    "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
    "            used for the linear transformation of the inputs. Default:\n",
    "            `\"glorot_uniform\"`.\n",
    "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
    "            weights matrix, used for the linear transformation\n",
    "            of the recurrent state. Default: `\"orthogonal\"`.\n",
    "        bias_initializer: Initializer for the bias vector. Default: `\"zeros\"`.\n",
    "        unit_forget_bias: Boolean (default `True`). If `True`,\n",
    "            add 1 to the bias of the forget gate at initialization.\n",
    "            Setting it to `True` will also force `bias_initializer=\"zeros\"`.\n",
    "            This is recommended in [Jozefowicz et al.](\n",
    "            https://github.com/mlresearch/v37/blob/gh-pages/jozefowicz15.pdf)\n",
    "        kernel_regularizer: Regularizer function applied to the `kernel` weights\n",
    "            matrix. Default: `None`.\n",
    "        recurrent_regularizer: Regularizer function applied to the\n",
    "            `recurrent_kernel` weights matrix. Default: `None`.\n",
    "        bias_regularizer: Regularizer function applied to the bias vector.\n",
    "            Default: `None`.\n",
    "        kernel_constraint: Constraint function applied to the `kernel` weights\n",
    "            matrix. Default: `None`.\n",
    "        recurrent_constraint: Constraint function applied to the\n",
    "            `recurrent_kernel` weights matrix. Default: `None`.\n",
    "        bias_constraint: Constraint function applied to the bias vector.\n",
    "            Default: `None`.\n",
    "        dropout: Float between 0 and 1. Fraction of the units to drop for the\n",
    "            linear transformation of the inputs. Default: 0.\n",
    "        recurrent_dropout: Float between 0 and 1. Fraction of the units to drop\n",
    "            for the linear transformation of the recurrent state. Default: 0.\n",
    "        seed: Random seed for dropout.\n",
    "\n",
    "    Call arguments:\n",
    "        inputs: A 2D tensor, with shape `(batch, features)`.\n",
    "        states: A 2D tensor with shape `(batch, units)`, which is the state\n",
    "            from the previous time step.\n",
    "        training: Python boolean indicating whether the layer should behave in\n",
    "            training mode or in inference mode. Only relevant when `dropout` or\n",
    "            `recurrent_dropout` is used.\n",
    "\n",
    "    Example:\n",
    "\n",
    "    >>> inputs = np.random.random((32, 10, 8))\n",
    "    >>> rnn = keras.layers.RNN(keras.layers.LSTMCell(4))\n",
    "    >>> output = rnn(inputs)\n",
    "    >>> output.shape\n",
    "    (32, 4)\n",
    "    >>> rnn = keras.layers.RNN(\n",
    "    ...    keras.layers.LSTMCell(4),\n",
    "    ...    return_sequences=True,\n",
    "    ...    return_state=True)\n",
    "    >>> whole_sequence_output, final_state = rnn(inputs)\n",
    "    >>> whole_sequence_output.shape\n",
    "    (32, 10, 4)\n",
    "    >>> final_state.shape\n",
    "    (32, 4)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "        use_bias=True,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        recurrent_initializer=\"orthogonal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        unit_forget_bias=True,\n",
    "        kernel_regularizer=None,\n",
    "        recurrent_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        recurrent_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        dropout=0.0,\n",
    "        recurrent_dropout=0.0,\n",
    "        seed=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if units <= 0:\n",
    "            raise ValueError(\n",
    "                \"Received an invalid value for argument `units`, \"\n",
    "                f\"expected a positive integer, got {units}.\"\n",
    "            )\n",
    "        implementation = kwargs.pop(\"implementation\", 2)\n",
    "        super().__init__(**kwargs)\n",
    "        self.implementation = implementation\n",
    "        self.units = units\n",
    "        self.activation = activations.get(activation)\n",
    "        self.recurrent_activation = activations.get(recurrent_activation)\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.recurrent_initializer = initializers.get(recurrent_initializer)\n",
    "        self.bias_initializer = initializers.get(bias_initializer)\n",
    "\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.recurrent_regularizer = regularizers.get(recurrent_regularizer)\n",
    "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
    "\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.recurrent_constraint = constraints.get(recurrent_constraint)\n",
    "        self.bias_constraint = constraints.get(bias_constraint)\n",
    "\n",
    "        self.dropout = min(1.0, max(0.0, dropout))\n",
    "        self.recurrent_dropout = min(1.0, max(0.0, recurrent_dropout))\n",
    "        if self.recurrent_dropout != 0.0:\n",
    "            self.implementation = 1\n",
    "        if self.implementation == 1:\n",
    "            self.dropout_mask_count = 4\n",
    "        self.seed = seed\n",
    "        self.seed_generator = backend.random.SeedGenerator(seed=seed)\n",
    "\n",
    "        self.unit_forget_bias = unit_forget_bias\n",
    "        self.state_size = [self.units, self.units]\n",
    "        self.output_size = self.units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        input_dim = input_shape[-1]\n",
    "        self.kernel = self.add_weight(\n",
    "            shape=(input_dim, self.units * 4),\n",
    "            name=\"kernel\",\n",
    "            initializer=self.kernel_initializer,\n",
    "            regularizer=self.kernel_regularizer,\n",
    "            constraint=self.kernel_constraint,\n",
    "        )\n",
    "        self.recurrent_kernel = self.add_weight(\n",
    "            shape=(self.units, self.units * 4),\n",
    "            name=\"recurrent_kernel\",\n",
    "            initializer=self.recurrent_initializer,\n",
    "            regularizer=self.recurrent_regularizer,\n",
    "            constraint=self.recurrent_constraint,\n",
    "        )\n",
    "\n",
    "        if self.use_bias:\n",
    "            if self.unit_forget_bias:\n",
    "\n",
    "                def bias_initializer(_, *args, **kwargs):\n",
    "                    return ops.concatenate(\n",
    "                        [\n",
    "                            self.bias_initializer(\n",
    "                                (self.units,), *args, **kwargs\n",
    "                            ),\n",
    "                            initializers.get(\"ones\")(\n",
    "                                (self.units,), *args, **kwargs\n",
    "                            ),\n",
    "                            self.bias_initializer(\n",
    "                                (self.units * 2,), *args, **kwargs\n",
    "                            ),\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "            else:\n",
    "                bias_initializer = self.bias_initializer\n",
    "            self.bias = self.add_weight(\n",
    "                shape=(self.units * 4,),\n",
    "                name=\"bias\",\n",
    "                initializer=bias_initializer,\n",
    "                regularizer=self.bias_regularizer,\n",
    "                constraint=self.bias_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.bias = None\n",
    "\n",
    "    def _compute_carry_and_output(self, x, h_tm1, c_tm1):\n",
    "        \"\"\"Computes carry and output using split kernels.\"\"\"\n",
    "        x_i, x_f, x_c, x_o = x\n",
    "        h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o = h_tm1\n",
    "        i = self.recurrent_activation(\n",
    "            x_i + ops.matmul(h_tm1_i, self.recurrent_kernel[:, : self.units])\n",
    "        )\n",
    "        f = self.recurrent_activation(\n",
    "            x_f\n",
    "            + ops.matmul(\n",
    "                h_tm1_f, self.recurrent_kernel[:, self.units : self.units * 2]\n",
    "            )\n",
    "        )\n",
    "        c = f * c_tm1 + i * self.activation(\n",
    "            x_c\n",
    "            + ops.matmul(\n",
    "                h_tm1_c,\n",
    "                self.recurrent_kernel[:, self.units * 2 : self.units * 3],\n",
    "            )\n",
    "        )\n",
    "        o = self.recurrent_activation(\n",
    "            x_o\n",
    "            + ops.matmul(h_tm1_o, self.recurrent_kernel[:, self.units * 3 :])\n",
    "        )\n",
    "        return c, o\n",
    "\n",
    "    def _compute_carry_and_output_fused(self, z, c_tm1):\n",
    "        \"\"\"Computes carry and output using fused kernels.\"\"\"\n",
    "        z0, z1, z2, z3 = z\n",
    "        i = self.recurrent_activation(z0)\n",
    "        f = self.recurrent_activation(z1)\n",
    "        c = f * c_tm1 + i * self.activation(z2)\n",
    "        o = self.recurrent_activation(z3)\n",
    "        return c, o\n",
    "\n",
    "    def call(self, inputs, states, training=False):\n",
    "        h_tm1 = states[0]  # previous memory state\n",
    "        c_tm1 = states[1]  # previous carry state\n",
    "\n",
    "        if self.implementation == 1:\n",
    "            if training and 0.0 < self.dropout < 1.0:\n",
    "                dp_mask = self.get_dropout_mask(inputs)\n",
    "                inputs_i = inputs * dp_mask[0]\n",
    "                inputs_f = inputs * dp_mask[1]\n",
    "                inputs_c = inputs * dp_mask[2]\n",
    "                inputs_o = inputs * dp_mask[3]\n",
    "            else:\n",
    "                inputs_i = inputs\n",
    "                inputs_f = inputs\n",
    "                inputs_c = inputs\n",
    "                inputs_o = inputs\n",
    "            k_i, k_f, k_c, k_o = ops.split(self.kernel, 4, axis=1)\n",
    "            x_i = ops.matmul(inputs_i, k_i)\n",
    "            x_f = ops.matmul(inputs_f, k_f)\n",
    "            x_c = ops.matmul(inputs_c, k_c)\n",
    "            x_o = ops.matmul(inputs_o, k_o)\n",
    "            if self.use_bias:\n",
    "                b_i, b_f, b_c, b_o = ops.split(self.bias, 4, axis=0)\n",
    "                x_i += b_i\n",
    "                x_f += b_f\n",
    "                x_c += b_c\n",
    "                x_o += b_o\n",
    "\n",
    "            if training and 0.0 < self.recurrent_dropout < 1.0:\n",
    "                rec_dp_mask = self.get_recurrent_dropout_mask(h_tm1)\n",
    "                h_tm1_i = h_tm1 * rec_dp_mask[0]\n",
    "                h_tm1_f = h_tm1 * rec_dp_mask[1]\n",
    "                h_tm1_c = h_tm1 * rec_dp_mask[2]\n",
    "                h_tm1_o = h_tm1 * rec_dp_mask[3]\n",
    "            else:\n",
    "                h_tm1_i = h_tm1\n",
    "                h_tm1_f = h_tm1\n",
    "                h_tm1_c = h_tm1\n",
    "                h_tm1_o = h_tm1\n",
    "            x = (x_i, x_f, x_c, x_o)\n",
    "            h_tm1 = (h_tm1_i, h_tm1_f, h_tm1_c, h_tm1_o)\n",
    "            c, o = self._compute_carry_and_output(x, h_tm1, c_tm1)\n",
    "        else:\n",
    "            if training and 0.0 < self.dropout < 1.0:\n",
    "                dp_mask = self.get_dropout_mask(inputs)\n",
    "                inputs = inputs * dp_mask\n",
    "\n",
    "            z = ops.matmul(inputs, self.kernel)\n",
    "\n",
    "            z += ops.matmul(h_tm1, self.recurrent_kernel)\n",
    "            if self.use_bias:\n",
    "                z += self.bias\n",
    "\n",
    "            z = ops.split(z, 4, axis=1)\n",
    "            c, o = self._compute_carry_and_output_fused(z, c_tm1)\n",
    "\n",
    "        h = o * self.activation(c)\n",
    "        return h, [h, c]\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"units\": self.units,\n",
    "            \"activation\": activations.serialize(self.activation),\n",
    "            \"recurrent_activation\": activations.serialize(\n",
    "                self.recurrent_activation\n",
    "            ),\n",
    "            \"use_bias\": self.use_bias,\n",
    "            \"unit_forget_bias\": self.unit_forget_bias,\n",
    "            \"kernel_initializer\": initializers.serialize(\n",
    "                self.kernel_initializer\n",
    "            ),\n",
    "            \"recurrent_initializer\": initializers.serialize(\n",
    "                self.recurrent_initializer\n",
    "            ),\n",
    "            \"bias_initializer\": initializers.serialize(self.bias_initializer),\n",
    "            \"kernel_regularizer\": regularizers.serialize(\n",
    "                self.kernel_regularizer\n",
    "            ),\n",
    "            \"recurrent_regularizer\": regularizers.serialize(\n",
    "                self.recurrent_regularizer\n",
    "            ),\n",
    "            \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\n",
    "            \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n",
    "            \"recurrent_constraint\": constraints.serialize(\n",
    "                self.recurrent_constraint\n",
    "            ),\n",
    "            \"bias_constraint\": constraints.serialize(self.bias_constraint),\n",
    "            \"dropout\": self.dropout,\n",
    "            \"recurrent_dropout\": self.recurrent_dropout,\n",
    "            \"seed\": self.seed,\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return {**base_config, **config}\n",
    "\n",
    "    def get_initial_state(self, batch_size=None):\n",
    "        return [\n",
    "            ops.zeros((batch_size, d), dtype=self.compute_dtype)\n",
    "            for d in self.state_size\n",
    "        ]\n",
    "\n",
    "\n",
    "@keras_export(\"keras.layers.LSTM\")\n",
    "class LSTM(RNN):\n",
    "    \"\"\"Long Short-Term Memory layer - Hochreiter 1997.\n",
    "\n",
    "    Based on available runtime hardware and constraints, this layer\n",
    "    will choose different implementations (cuDNN-based or backend-native)\n",
    "    to maximize the performance. If a GPU is available and all\n",
    "    the arguments to the layer meet the requirement of the cuDNN kernel\n",
    "    (see below for details), the layer will use a fast cuDNN implementation\n",
    "    when using the TensorFlow backend.\n",
    "    The requirements to use the cuDNN implementation are:\n",
    "\n",
    "    1. `activation` == `tanh`\n",
    "    2. `recurrent_activation` == `sigmoid`\n",
    "    3. `recurrent_dropout` == 0\n",
    "    4. `unroll` is `False`\n",
    "    5. `use_bias` is `True`\n",
    "    6. Inputs, if use masking, are strictly right-padded.\n",
    "    7. Eager execution is enabled in the outermost context.\n",
    "\n",
    "    For example:\n",
    "\n",
    "    >>> inputs = np.random.random((32, 10, 8))\n",
    "    >>> lstm = keras.layers.LSTM(4)\n",
    "    >>> output = lstm(inputs)\n",
    "    >>> output.shape\n",
    "    (32, 4)\n",
    "    >>> lstm = keras.layers.LSTM(\n",
    "    ...     4, return_sequences=True, return_state=True)\n",
    "    >>> whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)\n",
    "    >>> whole_seq_output.shape\n",
    "    (32, 10, 4)\n",
    "    >>> final_memory_state.shape\n",
    "    (32, 4)\n",
    "    >>> final_carry_state.shape\n",
    "    (32, 4)\n",
    "\n",
    "    Args:\n",
    "        units: Positive integer, dimensionality of the output space.\n",
    "        activation: Activation function to use.\n",
    "            Default: hyperbolic tangent (`tanh`).\n",
    "            If you pass `None`, no activation is applied\n",
    "            (ie. \"linear\" activation: `a(x) = x`).\n",
    "        recurrent_activation: Activation function to use\n",
    "            for the recurrent step.\n",
    "            Default: sigmoid (`sigmoid`).\n",
    "            If you pass `None`, no activation is applied\n",
    "            (ie. \"linear\" activation: `a(x) = x`).\n",
    "        use_bias: Boolean, (default `True`), whether the layer\n",
    "            should use a bias vector.\n",
    "        kernel_initializer: Initializer for the `kernel` weights matrix,\n",
    "            used for the linear transformation of the inputs. Default:\n",
    "            `\"glorot_uniform\"`.\n",
    "        recurrent_initializer: Initializer for the `recurrent_kernel`\n",
    "            weights matrix, used for the linear transformation of the recurrent\n",
    "            state. Default: `\"orthogonal\"`.\n",
    "        bias_initializer: Initializer for the bias vector. Default: `\"zeros\"`.\n",
    "        unit_forget_bias: Boolean (default `True`). If `True`,\n",
    "            add 1 to the bias of the forget gate at initialization.\n",
    "            Setting it to `True` will also force `bias_initializer=\"zeros\"`.\n",
    "            This is recommended in [Jozefowicz et al.](\n",
    "            https://github.com/mlresearch/v37/blob/gh-pages/jozefowicz15.pdf)\n",
    "        kernel_regularizer: Regularizer function applied to the `kernel` weights\n",
    "            matrix. Default: `None`.\n",
    "        recurrent_regularizer: Regularizer function applied to the\n",
    "            `recurrent_kernel` weights matrix. Default: `None`.\n",
    "        bias_regularizer: Regularizer function applied to the bias vector.\n",
    "            Default: `None`.\n",
    "        activity_regularizer: Regularizer function applied to the output of the\n",
    "            layer (its \"activation\"). Default: `None`.\n",
    "        kernel_constraint: Constraint function applied to the `kernel` weights\n",
    "            matrix. Default: `None`.\n",
    "        recurrent_constraint: Constraint function applied to the\n",
    "            `recurrent_kernel` weights matrix. Default: `None`.\n",
    "        bias_constraint: Constraint function applied to the bias vector.\n",
    "            Default: `None`.\n",
    "        dropout: Float between 0 and 1. Fraction of the units to drop for the\n",
    "            linear transformation of the inputs. Default: 0.\n",
    "        recurrent_dropout: Float between 0 and 1. Fraction of the units to drop\n",
    "            for the linear transformation of the recurrent state. Default: 0.\n",
    "        seed: Random seed for dropout.\n",
    "        return_sequences: Boolean. Whether to return the last output\n",
    "            in the output sequence, or the full sequence. Default: `False`.\n",
    "        return_state: Boolean. Whether to return the last state in addition\n",
    "            to the output. Default: `False`.\n",
    "        go_backwards: Boolean (default: `False`).\n",
    "            If `True`, process the input sequence backwards and return the\n",
    "            reversed sequence.\n",
    "        stateful: Boolean (default: `False`). If `True`, the last state\n",
    "            for each sample at index i in a batch will be used as initial\n",
    "            state for the sample of index i in the following batch.\n",
    "        unroll: Boolean (default False).\n",
    "            If `True`, the network will be unrolled,\n",
    "            else a symbolic loop will be used.\n",
    "            Unrolling can speed-up a RNN,\n",
    "            although it tends to be more memory-intensive.\n",
    "            Unrolling is only suitable for short sequences.\n",
    "        use_cudnn: Whether to use a cuDNN-backed implementation. `\"auto\"` will\n",
    "            attempt to use cuDNN when feasible, and will fallback to the\n",
    "            default implementation if not.\n",
    "\n",
    "    Call arguments:\n",
    "        inputs: A 3D tensor, with shape `(batch, timesteps, feature)`.\n",
    "        mask: Binary tensor of shape `(samples, timesteps)` indicating whether\n",
    "            a given timestep should be masked  (optional).\n",
    "            An individual `True` entry indicates that the corresponding timestep\n",
    "            should be utilized, while a `False` entry indicates that the\n",
    "            corresponding timestep should be ignored. Defaults to `None`.\n",
    "        training: Python boolean indicating whether the layer should behave in\n",
    "            training mode or in inference mode. This argument is passed to the\n",
    "            cell when calling it. This is only relevant if `dropout` or\n",
    "            `recurrent_dropout` is used  (optional). Defaults to `None`.\n",
    "        initial_state: List of initial state tensors to be passed to the first\n",
    "            call of the cell (optional, `None` causes creation\n",
    "            of zero-filled initial state tensors). Defaults to `None`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        units,\n",
    "        activation=\"tanh\",\n",
    "        recurrent_activation=\"sigmoid\",\n",
    "        use_bias=True,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        recurrent_initializer=\"orthogonal\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        unit_forget_bias=True,\n",
    "        kernel_regularizer=None,\n",
    "        recurrent_regularizer=None,\n",
    "        bias_regularizer=None,\n",
    "        activity_regularizer=None,\n",
    "        kernel_constraint=None,\n",
    "        recurrent_constraint=None,\n",
    "        bias_constraint=None,\n",
    "        dropout=0.0,\n",
    "        recurrent_dropout=0.0,\n",
    "        seed=None,\n",
    "        return_sequences=False,\n",
    "        return_state=False,\n",
    "        go_backwards=False,\n",
    "        stateful=False,\n",
    "        unroll=False,\n",
    "        use_cudnn=\"auto\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        cell = LSTMCell(\n",
    "            units,\n",
    "            activation=activation,\n",
    "            recurrent_activation=recurrent_activation,\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=kernel_initializer,\n",
    "            unit_forget_bias=unit_forget_bias,\n",
    "            recurrent_initializer=recurrent_initializer,\n",
    "            bias_initializer=bias_initializer,\n",
    "            kernel_regularizer=kernel_regularizer,\n",
    "            recurrent_regularizer=recurrent_regularizer,\n",
    "            bias_regularizer=bias_regularizer,\n",
    "            kernel_constraint=kernel_constraint,\n",
    "            recurrent_constraint=recurrent_constraint,\n",
    "            bias_constraint=bias_constraint,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            dtype=kwargs.get(\"dtype\", None),\n",
    "            trainable=kwargs.get(\"trainable\", True),\n",
    "            name=\"lstm_cell\",\n",
    "            seed=seed,\n",
    "            implementation=kwargs.pop(\"implementation\", 2),\n",
    "        )\n",
    "        super().__init__(\n",
    "            cell,\n",
    "            return_sequences=return_sequences,\n",
    "            return_state=return_state,\n",
    "            go_backwards=go_backwards,\n",
    "            stateful=stateful,\n",
    "            unroll=unroll,\n",
    "            activity_regularizer=activity_regularizer,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "        if use_cudnn not in (\"auto\", True, False):\n",
    "            raise ValueError(\n",
    "                \"Invalid valid received for argument `use_cudnn`. \"\n",
    "                \"Expected one of {'auto', True, False}. \"\n",
    "                f\"Received: use_cudnn={use_cudnn}\"\n",
    "            )\n",
    "        self.use_cudnn = use_cudnn\n",
    "        if (\n",
    "            backend.backend() == \"tensorflow\"\n",
    "            and backend.cudnn_ok(\n",
    "                cell.activation,\n",
    "                cell.recurrent_activation,\n",
    "                self.unroll,\n",
    "                cell.use_bias,\n",
    "            )\n",
    "            and use_cudnn in (True, \"auto\")\n",
    "        ):\n",
    "            self.supports_jit = False\n",
    "\n",
    "    def inner_loop(self, sequences, initial_state, mask, training=False):\n",
    "        if tree.is_nested(mask):\n",
    "            mask = mask[0]\n",
    "\n",
    "        if self.use_cudnn in (\"auto\", True):\n",
    "            if not self.recurrent_dropout:\n",
    "                try:\n",
    "                    if training and self.dropout:\n",
    "                        dp_mask = self.cell.get_dropout_mask(sequences[:, 0, :])\n",
    "                        dp_mask = ops.expand_dims(dp_mask, axis=1)\n",
    "                        dp_mask = ops.broadcast_to(\n",
    "                            dp_mask, ops.shape(sequences)\n",
    "                        )\n",
    "                        dp_sequences = sequences * dp_mask\n",
    "                    else:\n",
    "                        dp_sequences = sequences\n",
    "\n",
    "                    # Backends are allowed to specify (optionally) optimized\n",
    "                    # implementation of the inner LSTM loop. In the case of\n",
    "                    # TF for instance, it will leverage cuDNN when feasible, and\n",
    "                    # it will raise NotImplementedError otherwise.\n",
    "                    out = backend.lstm(\n",
    "                        dp_sequences,\n",
    "                        initial_state[0],\n",
    "                        initial_state[1],\n",
    "                        mask,\n",
    "                        kernel=self.cell.kernel,\n",
    "                        recurrent_kernel=self.cell.recurrent_kernel,\n",
    "                        bias=self.cell.bias,\n",
    "                        activation=self.cell.activation,\n",
    "                        recurrent_activation=self.cell.recurrent_activation,\n",
    "                        return_sequences=self.return_sequences,\n",
    "                        go_backwards=self.go_backwards,\n",
    "                        unroll=self.unroll,\n",
    "                    )\n",
    "                    # We disable jit_compile for the model in this case,\n",
    "                    # since cuDNN ops aren't XLA compatible.\n",
    "                    if backend.backend() == \"tensorflow\":\n",
    "                        self.supports_jit = False\n",
    "                    return out\n",
    "                except NotImplementedError:\n",
    "                    pass\n",
    "        if self.use_cudnn is True:\n",
    "            raise ValueError(\n",
    "                \"use_cudnn=True was specified, \"\n",
    "                \"but cuDNN is not supported for this layer configuration \"\n",
    "                \"with this backend. Pass use_cudnn='auto' to fallback \"\n",
    "                \"to a non-cuDNN implementation.\"\n",
    "            )\n",
    "        return super().inner_loop(\n",
    "            sequences, initial_state, mask=mask, training=training\n",
    "        )\n",
    "\n",
    "    def call(self, sequences, initial_state=None, mask=None, training=False):\n",
    "        return super().call(\n",
    "            sequences, mask=mask, training=training, initial_state=initial_state\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def units(self):\n",
    "        return self.cell.units\n",
    "\n",
    "    @property\n",
    "    def activation(self):\n",
    "        return self.cell.activation\n",
    "\n",
    "    @property\n",
    "    def recurrent_activation(self):\n",
    "        return self.cell.recurrent_activation\n",
    "\n",
    "    @property\n",
    "    def use_bias(self):\n",
    "        return self.cell.use_bias\n",
    "\n",
    "    @property\n",
    "    def unit_forget_bias(self):\n",
    "        return self.cell.unit_forget_bias\n",
    "\n",
    "    @property\n",
    "    def kernel_initializer(self):\n",
    "        return self.cell.kernel_initializer\n",
    "\n",
    "    @property\n",
    "    def recurrent_initializer(self):\n",
    "        return self.cell.recurrent_initializer\n",
    "\n",
    "    @property\n",
    "    def bias_initializer(self):\n",
    "        return self.cell.bias_initializer\n",
    "\n",
    "    @property\n",
    "    def kernel_regularizer(self):\n",
    "        return self.cell.kernel_regularizer\n",
    "\n",
    "    @property\n",
    "    def recurrent_regularizer(self):\n",
    "        return self.cell.recurrent_regularizer\n",
    "\n",
    "    @property\n",
    "    def bias_regularizer(self):\n",
    "        return self.cell.bias_regularizer\n",
    "\n",
    "    @property\n",
    "    def kernel_constraint(self):\n",
    "        return self.cell.kernel_constraint\n",
    "\n",
    "    @property\n",
    "    def recurrent_constraint(self):\n",
    "        return self.cell.recurrent_constraint\n",
    "\n",
    "    @property\n",
    "    def bias_constraint(self):\n",
    "        return self.cell.bias_constraint\n",
    "\n",
    "    @property\n",
    "    def dropout(self):\n",
    "        return self.cell.dropout\n",
    "\n",
    "    @property\n",
    "    def recurrent_dropout(self):\n",
    "        return self.cell.recurrent_dropout\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"units\": self.units,\n",
    "            \"activation\": activations.serialize(self.activation),\n",
    "            \"recurrent_activation\": activations.serialize(\n",
    "                self.recurrent_activation\n",
    "            ),\n",
    "            \"use_bias\": self.use_bias,\n",
    "            \"kernel_initializer\": initializers.serialize(\n",
    "                self.kernel_initializer\n",
    "            ),\n",
    "            \"recurrent_initializer\": initializers.serialize(\n",
    "                self.recurrent_initializer\n",
    "            ),\n",
    "            \"bias_initializer\": initializers.serialize(self.bias_initializer),\n",
    "            \"unit_forget_bias\": self.unit_forget_bias,\n",
    "            \"kernel_regularizer\": regularizers.serialize(\n",
    "                self.kernel_regularizer\n",
    "            ),\n",
    "            \"recurrent_regularizer\": regularizers.serialize(\n",
    "                self.recurrent_regularizer\n",
    "            ),\n",
    "            \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\n",
    "            \"activity_regularizer\": regularizers.serialize(\n",
    "                self.activity_regularizer\n",
    "            ),\n",
    "            \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n",
    "            \"recurrent_constraint\": constraints.serialize(\n",
    "                self.recurrent_constraint\n",
    "            ),\n",
    "            \"bias_constraint\": constraints.serialize(self.bias_constraint),\n",
    "            \"dropout\": self.dropout,\n",
    "            \"recurrent_dropout\": self.recurrent_dropout,\n",
    "            \"seed\": self.cell.seed,\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        del base_config[\"cell\"]\n",
    "        return {**base_config, **config}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
